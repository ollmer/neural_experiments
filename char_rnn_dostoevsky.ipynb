{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM,TimeDistributed, Embedding\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.optimizers import RMSprop, Adam, SGD\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 9469628\n"
     ]
    }
   ],
   "source": [
    "fname = 'dostoevsky.txt'\n",
    "text = open(fname, 'r', encoding='utf-8').read()\n",
    "print('corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique chars: 162\n",
      "unique bigrams: 3566\n",
      "unique trigrams: 26082\n",
      "word count: 1528727\n",
      "unique words: 165930\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print('unique chars:', len(chars))\n",
    "char_idx = dict((c, i) for i, c in enumerate(chars))\n",
    "idx_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "bigrams = []\n",
    "for i in range(0, len(text)-1, 1):\n",
    "    bigrams.append(text[i] + text[i+1])\n",
    "bigrams = sorted(list(set(bigrams)))\n",
    "print('unique bigrams:', len(bigrams))\n",
    "bigram_idx = dict((c, i) for i, c in enumerate(bigrams))\n",
    "idx_bigram = dict((i, c) for i, c in enumerate(bigrams))\n",
    "\n",
    "trigrams = []\n",
    "for i in range(0, len(text)-2, 1):\n",
    "    trigrams.append(text[i] + text[i+1] + text[i+2])\n",
    "trigrams = sorted(list(set(trigrams)))\n",
    "print('unique trigrams:', len(trigrams))\n",
    "trigram_idx = dict((c, i) for i, c in enumerate(trigrams))\n",
    "idx_trigram = dict((i, c) for i, c in enumerate(trigrams))\n",
    "\n",
    "words = text.split()\n",
    "print('word count:', len(words))\n",
    "words = sorted(list(set(words)))\n",
    "print('unique words:', len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " только, что сын его, воспитывавшийся сначала у графа, а потом в лицее, окончил тогда курс наук девятнадцати лет от роду. Я написал об этом к Ихменевым, а также и о том, что князь очень любит своего сына, балует его, рассчитывает уже и теперь его будущность. Всё это я узнал от товарищей-студентов, знакомых молодому князю. В это-то время Николай Сергеич в одно прекрасное утро получил от князя письмо, чрезвычайно его удивившее...\n",
      "   Князь, который до сих пор, как уже упомянул я, ограничивался в сношениях с Николаем Сергеичем одной сухой, деловой перепиской, писал к нему теперь самым подробным, откровенным и дружеским образом о своих семейных обстоятельствах: он жаловался на своего сына, писал, что сын огорчает его дурным своим поведением; что, конечно, на шалости такого мальчика нельзя еще смотреть слишком серьезно (он видимо старался оправдать его), но что он решился наказать сына, попугать его, а именно: сослать ого на некоторое время в деревню, под присмотр Ихменева. Князь писал, что \n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024 # decrease if you have \"Failed to allocate memory\" error when start training\n",
    "track_size = len(text) // batch_size\n",
    "tracks = ['' for i in range(batch_size)]\n",
    "for i in range(0, track_size):\n",
    "    for track in range(batch_size):\n",
    "        tracks[track] += text[track * track_size + i]\n",
    "\n",
    "# Let's see what we've got\n",
    "print(tracks[4][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary done:  162\n"
     ]
    }
   ],
   "source": [
    "gram_idx = [char_idx, bigram_idx, trigram_idx]\n",
    "idx_gram = [idx_char, idx_bigram, idx_trigram]\n",
    "\n",
    "#converts to indexes of chars, bigrams, trigrams\n",
    "def grams(tracks, n=1):\n",
    "    indexed = []\n",
    "    for t in tracks:  \n",
    "        track = []\n",
    "        for i in range(0, len(t)-n+1, n):\n",
    "            gram = ''\n",
    "            for j in range(n):\n",
    "                gram += t[i+j]\n",
    "            idx = gram_idx[n-1][gram]\n",
    "            track.append(idx)\n",
    "        indexed.append(track)\n",
    "    return indexed\n",
    "\n",
    "indexed = grams(tracks, 1)\n",
    "vocab_size = len(gram_idx[0])\n",
    "print('Vocabulary done: ', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples 236544\n",
      "Number of training labels 236544\n",
      "Label sequence length 40\n",
      "Label character one-hot vector length 162\n"
     ]
    }
   ],
   "source": [
    "seq_len = 40 #length of sequence in a batch\n",
    "\n",
    "def onehot(n):\n",
    "    v = [0 for i in range(vocab_size)]\n",
    "    v[n] = 1\n",
    "    return v\n",
    "\n",
    "#build order for batches for stateful lstm\n",
    "def vectorize(tracks):\n",
    "    track_size = len(tracks[0])\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(0, track_size - seq_len + 1, seq_len):\n",
    "        for t in tracks:\n",
    "            X.append(t[i:i + seq_len])\n",
    "            target = [onehot(c) for c in t[i+1:i + seq_len + 1]]\n",
    "            y.append(target)\n",
    "    return X, y\n",
    "        \n",
    "x,y = vectorize(indexed)\n",
    "print('Number of training samples', len(x))\n",
    "print('Number of training labels', len(y))\n",
    "print('Label sequence length', len(y[0]))\n",
    "print('Label character one-hot vector length', len(y[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1024, 40, 30)            4860      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (1024, 40, 512)           1112064   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (1024, 40, 512)           2099200   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1024, 40, 162)           83106     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (1024, 40, 162)           0         \n",
      "=================================================================\n",
      "Total params: 3,299,230\n",
      "Trainable params: 3,299,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cells = 512\n",
    "drop = 0.2\n",
    "embed = 30 # size of character embedding\n",
    "layers = 2\n",
    "lr = 0.01\n",
    "clip = 5.0 # gradient clipping to prevent exploding gradients\n",
    "stateful=True # maintain layer state between batches\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed, batch_input_shape=(batch_size, seq_len)))\n",
    "for l in range(layers):\n",
    "    model.add(LSTM(cells, return_sequences=True, stateful=stateful, dropout=drop))\n",
    "model.add(Dense(vocab_size))\n",
    "model.add(Activation('softmax'))\n",
    "optimizer = Adam(lr, clipnorm=clip)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_callbacks(filepath, patience=5):\n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor='loss', \n",
    "                                            patience=patience, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)\n",
    "    es = EarlyStopping('loss', verbose=1, min_delta=0.02, patience=patience, mode=\"min\")\n",
    "    return [learning_rate_reduction, es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=0.5):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictive_model(main_model): # change batch size to 1 for work with one sequence\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embed, batch_input_shape=(1, seq_len)))\n",
    "    for l in range(layers):\n",
    "        model.add(LSTM(cells, return_sequences=True, stateful=stateful, dropout=drop))\n",
    "    model.add(Dense(vocab_size))\n",
    "    model.add(Activation('softmax'))\n",
    "    optimizer = Adam(lr, clipnorm=clip)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
    "    old_weights = main_model.get_weights()\n",
    "    model.set_weights(old_weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate new chars from model\n",
    "def test(model, l=500, seed = None, t=0.5):\n",
    "    start_from = np.random.randint(len(text)-seq_len)+seq_len\n",
    "    seed_string = text[start_from:start_from + seq_len*3] if seed is None else seed\n",
    "    print('\\n\\nSeed:  ', seed_string)\n",
    "    print('----')\n",
    "    sys.stdout.write(seed_string)\n",
    "    prmodel = predictive_model(model)\n",
    "    for i in range(l):\n",
    "        prmodel.reset_states()\n",
    "        padlen = (len(seed_string) // seq_len +1) * seq_len\n",
    "        seed_string = seed_string.rjust(padlen)[-seq_len*3:]\n",
    "        test_tracks = [seed_string]\n",
    "        tidx = grams(test_tracks)\n",
    "        xt, _ = vectorize(tidx)\n",
    "        preds = prmodel.predict(np.array(xt), batch_size=1, verbose=0)\n",
    "        preds = preds[-1][-1] # last symbol of last sequence\n",
    "        next_item = idx_char[sample(preds, t)]\n",
    "        seed_string = seed_string + next_item\n",
    "        sys.stdout.write(next_item)\n",
    "        sys.stdout.flush()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 1\n",
      "Epoch 1/1\n",
      "236544/236544 [==============================] - 196s - loss: 3.3098   \n",
      "\n",
      "Iteration 2\n",
      "Epoch 1/1\n",
      "236544/236544 [==============================] - 188s - loss: 3.1676   \n",
      "\n",
      "Iteration 3\n",
      "Epoch 1/1\n",
      "236544/236544 [==============================] - 191s - loss: 2.5591   \n",
      ">>>  икальным объяснением, несмотря на то что дело плевое; я знаю его еще с Петербурга. К тому же весь анекдот делает только \n",
      "----\n",
      "икальным объяснением, несмотря на то что дело плевое; я знаю его еще с Петербурга. К тому же весь анекдот делает только на не воглану и вот двадь не сопривила не на прокоть пробовореться на солать не могда уго всего же самовеле дервать,  потодал\n",
      " что старет придорил отанила спа не прогость же того и не сот отводил все ста стастве. Прегавове,  скоронить е всё логда на с нимененно послей на проемала не не стольто вы мне мни сам, кня на старате телова пословаеть так на на смество миже все преседь слуго все в неста не сами призчите вот, не на тогда не закорорно как трязь потисту сопросенить лестольно из семи его на с\n",
      "Iteration 4\n",
      "Epoch 1/1\n",
      "236544/236544 [==============================] - 186s - loss: 2.0483   \n",
      "\n",
      "Iteration 5\n",
      "Epoch 1/1\n",
      "236544/236544 [==============================] - 190s - loss: 1.8013   \n",
      "\n",
      "Iteration 6\n",
      "Epoch 1/1\n",
      "236544/236544 [==============================] - 190s - loss: 1.6861   \n",
      ">>>   вдруг неожиданно.\n",
      "   - Как тем хуже?\n",
      "   - Хуже.\n",
      "   - Не понимаю.\n",
      "   - Друг мой, друг мой, ну пусть в Сибирь, в Архангел\n",
      "----\n",
      " вдруг неожиданно.\n",
      "   - Как тем хуже?\n",
      "   - Хуже.\n",
      "   - Не понимаю.\n",
      "   - Друг мой, друг мой, ну пусть в Сибирь, в Архангела Ивановна. То не по своего подсудим на дворе, что так, все все хоть положения и все это было вопросы всё просто под восторгами на него на вами спросил в говорить и в этом вашим с длинным себя. Вы вы ответил в демовым столько не могла какого не верите служал же довольно под всех совершенно закричала, что только было вы в комнате в нем до всеми продолжал и вдруг продолжал было не было образом деле не почему-то с первого не знал, вот тогда уж не знаю, точно так ли сильно в ней положительно всей не\n",
      "Iteration 7\n",
      "Epoch 1/1\n",
      "236544/236544 [==============================] - 188s - loss: 1.6235   \n",
      "\n",
      "Iteration 8\n",
      "Epoch 1/1\n",
      "236544/236544 [==============================] - 188s - loss: 1.5817   \n",
      "\n",
      "Iteration 9\n",
      "Epoch 1/1\n",
      "236544/236544 [==============================] - 189s - loss: 1.5523   \n",
      ">>>  в монастырские ворота пешком. Кроме Федора Павловича, остальные трое кажется никогда не видали никакого монастыря, а Миу\n",
      "----\n",
      "в монастырские ворота пешком. Кроме Федора Павловича, остальные трое кажется никогда не видали никакого монастыря, а Миусья от дверь обратил весь не полицей, слышал и с тобой на своей нетерпением про себя навсегда в самом долго и получил на того, которого же со столу просто под воротились к деревне, то есть до сих в креста, но не в полу примерника и не потому что тотчас же было не мог подле после него с неестественное все это самовольно и во всем доме проговорил с нелепый и и ответила в каком-то любопытством своего например, что всё равно не только и от после последней стола. Вот по свою для него последнее сторон\n",
      "Iteration 10\n",
      "Epoch 1/1\n",
      "236544/236544 [==============================] - 190s - loss: 1.5294   \n",
      "\n",
      "Iteration 11\n",
      "Epoch 1/1\n",
      "236544/236544 [==============================] - 193s - loss: 1.5108   \n",
      "\n",
      "Iteration 12\n",
      "Epoch 1/1\n",
      "236544/236544 [==============================] - 175s - loss: 1.4962   \n",
      ">>>  чал и долго обдумывал.\n",
      "   -- Штука в том: я задал себе один раз такой вопрос: что если бы, например, на моем месте случи\n",
      "----\n",
      "чал и долго обдумывал.\n",
      "   -- Штука в том: я задал себе один раз такой вопрос: что если бы, например, на моем месте случилось и даже до сих пор не было уже, чтобы принял его на \"собственных сил за красного стороны, по приходить явился на покойным волнением уверена нахмурился и что он все до рассказывался к нему на полным положении. Пойдемте, она заплачения и не мог от всех пред том и уже не знает и может, с негодованиями человеком, по крайней больше и открылась от меня и умел в болезни послали. Он не простить на меня в однем доме подумали его. Если бы он в том, что вы из него он был в горячее и подле обойхнулся в \n",
      "Iteration 13\n",
      "Epoch 1/1\n",
      "236544/236544 [==============================] - 133s - loss: 1.4842   \n",
      "\n",
      "Iteration 14\n",
      "Epoch 1/1\n",
      "236544/236544 [==============================] - 133s - loss: 1.4732   \n",
      "\n",
      "Iteration 15\n",
      "Epoch 1/1\n",
      "236544/236544 [==============================] - 132s - loss: 1.4645   \n",
      ">>>   Стоило ли это теперь хоть какой-нибудь тревоги, в свою очередь, хотя какого-нибудь даже внимания! Он стоял, читал, слуш\n",
      "----\n",
      " Стоило ли это теперь хоть какой-нибудь тревоги, в свою очередь, хотя какого-нибудь даже внимания! Он стоял, читал, слушают из всех пор не потом и старика и бросились о том, что он Лебядкина была сел в комнате с нею с дверью собственные вечер в господин.\n",
      "   -- Нет, не знаю на красного любовь этого была от него в самом великодушным предмете, и не верите меня с тобой, и не подумал в комнате, но он высказал все два стороны совсем не случилось в голову. Вы тоже после принести и не потому что просто не хочу! -- спросил он вдруг в ту же словах на меня на него и не видел и уже не буду повернулась было слевение. Нарочно \n",
      "Iteration 16\n",
      "Epoch 1/1\n",
      "236544/236544 [==============================] - 131s - loss: 1.4558   \n",
      "\n",
      "Iteration 17\n",
      "Epoch 1/1\n",
      "236544/236544 [==============================] - 132s - loss: 1.4484   \n",
      "\n",
      "Iteration 18\n",
      "Epoch 1/1\n",
      "236544/236544 [==============================] - 132s - loss: 1.4423   \n",
      ">>>   мне было почему-то ужасно совестно заговаривать о Полине; он же сам ни слова о ней не спросил. Я рассказал ему про бабу\n",
      "----\n",
      " мне было почему-то ужасно совестно заговаривать о Полине; он же сам ни слова о ней не спросил. Я рассказал ему про бабушкой и познакомился образованным под всего последнею голову со голову. Но не понял гостя смотрела на него и про то не обернулся и между тем и больше настоящий и все равно обратилась на два доктор и на положения и не получил на него увидеть на этот раз ответил и должно быть, потому что в этот раз так сказать, и уж не прошептал бы не понять, что она всегда все спокойно прокурор вздрагал за своей руку. Он обойтись, как бы сказать, и он пристально поступила на меня наконец, подхватив перепалительно \n",
      "Iteration 19\n",
      "Epoch 1/1\n",
      "236544/236544 [==============================] - 132s - loss: 1.4355   \n",
      "\n",
      "Iteration 20\n",
      "Epoch 1/1\n",
      "236544/236544 [==============================] - 133s - loss: 1.4321   \n",
      "\n",
      "Iteration 21\n",
      "Epoch 1/1\n",
      "236544/236544 [==============================] - 134s - loss: 1.4265   \n",
      ">>>  ра кончить!\n",
      "   -- Объяснитесь, Наталья Николаевна, -- подхватил князь, -- убедительно прошу вас! Я уже два часа слышу об\n",
      "----\n",
      "ра кончить!\n",
      "   -- Объяснитесь, Наталья Николаевна, -- подхватил князь, -- убедительно прошу вас! Я уже два часа слышу обо всем даже теперь в этом обществе и слушать, что он совсем не знал \"на нее\". Но все могу от него была в том, что назад на него с первого взгляда по крайней мере всегда так всегда подлецой, что не подробно стал в таком половину и с нею ни за что и не сказал его с тем и стало быть все время прошло подумал, что только что она была только хоть и положительно стала допустить глазами, -- подумал он вдруг он мелькнула и ответила в своей месте в беспокойстве в дела. Поверьте, что и может быть, не думаю\n",
      "Iteration 22\n",
      "Epoch 1/1\n",
      "236544/236544 [==============================] - 132s - loss: 1.4226   \n",
      "\n",
      "Iteration 23\n",
      "Epoch 1/1\n",
      "236544/236544 [==============================] - 131s - loss: 1.4182   \n",
      "\n",
      "Iteration 24\n",
      "Epoch 1/1\n",
      "236544/236544 [==============================] - 129s - loss: 1.4148   \n",
      ">>>  ила Авдотья Романовна.\n",
      "   -- Вы думаете, -- с жаром продолжала Пульхерия Александровна, -- его бы остановили тогда мои с\n",
      "----\n",
      "ила Авдотья Романовна.\n",
      "   -- Вы думаете, -- с жаром продолжала Пульхерия Александровна, -- его бы остановили тогда мои старухи не брать предложение просто подумал, что по крайней мере в моем деле и с тем на него гордости в другой раз, что он не потому в безородном отца в первый раз был не подумали. Пусть на меня показалось в недоумении на ней и вышел к нему. Потому что так на него надо было не надо было сказать на делам и подсудимого и в подлец свои лет в половину в то же моей себе его с первого последнее пользу на меня в показать себя с свою неужели и в нем повторять общество. Он понял теперь в глаза от двух под\n",
      "Iteration 25\n",
      "Epoch 1/1\n",
      "236544/236544 [==============================] - 133s - loss: 1.4118   \n",
      "\n",
      "Iteration 26\n",
      "Epoch 1/1\n",
      "236544/236544 [==============================] - 178s - loss: 1.4093   \n",
      "\n",
      "Iteration 27\n",
      "Epoch 1/1\n",
      "236544/236544 [==============================] - 135s - loss: 1.4065   \n",
      ">>>  рены, благодушнейший, искреннейший и благороднейший князь, -- вскричал Лебедев в решительном вдохновении, -- будьте увер\n",
      "----\n",
      "рены, благодушнейший, искреннейший и благороднейший князь, -- вскричал Лебедев в решительном вдохновении, -- будьте уверяли ваше минуты, -- как тот с нею моим добрым с лестнице. И вот я не обращаюсь, что я ведь не отвечал и пристально поступил и понял, что я только уж как я не стал все и так по лежать на восторге в городе и по крайней мере даже не со мной. Но он ведь только не имел в таком случае и при этом странной стороны, но вы его даже не поставить. А он потому что я еще не понимаешь, что я как бы не знаю, что он в нем долгими с тобой в перед нею и не видал его в руках и последних старуха и все это удивлялись\n",
      "Iteration 28\n",
      "Epoch 1/1\n",
      "236544/236544 [==============================] - 137s - loss: 1.4029   \n"
     ]
    }
   ],
   "source": [
    "for iteration in range(1, 51):\n",
    "    print('\\nIteration', iteration)\n",
    "    model_name = 'char_%s_%d_%d_%.1f_%d.h5' % (fname, layers, cells, drop, iteration)\n",
    "    history=model.fit(\n",
    "        np.array(x), np.array(y), \n",
    "        batch_size=batch_size, \n",
    "        epochs=1, \n",
    "        verbose=1, \n",
    "        shuffle=False,\n",
    "        callbacks=get_callbacks(filepath=model_name)\n",
    "    )\n",
    "    model.save_weights(model_name, overwrite=True)\n",
    "    model.reset_states()\n",
    "    if iteration%3 == 0:\n",
    "        test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Seed:   У нас в Малом зале до сих пор проходят\n",
      "----\n",
      "У нас в Малом зале до сих пор проходят с ним с тобой и подозревали и подозревали и при всех столь и под конец просто подозрения и просто подозревали и при своей стороны и принять своего простодушного старика и подозревали и просто подозревали и подозревали и под конец в своем положении и принять в себе старика, но в том, что он всегда п\n",
      "\n",
      "Seed:   У нас в Малом зале до сих пор проходят\n",
      "----\n",
      "У нас в Малом зале до сих пор проходят на свою разом и не после прежнего правда с тревожностью, выразился и смотрел на положении. Но просто отвечала она принять меня и сказал его и пред нею, как бы во всех своего проклятий принес в картину и стал простодушное ветром и воздух не понимаю, что не давать предстоящего и судорога. Она убил с \n",
      "\n",
      "Seed:   У нас в Малом зале до сих пор проходят\n",
      "----\n",
      "У нас в Малом зале до сих пор проходят картины. Теперь сейчас они остановившись пошле дорогу совсем и почти неожиданное  своего  затрастно   на  собой  в  том,  чтобы  не\n",
      "  сказали его ответ к нему прочтить виде и деньги в руках голова  -  учительно  и\n",
      "  старику с каким-то полячной столы погросить, но на него. -- Всё больше-то и распоря"
     ]
    }
   ],
   "source": [
    "seed = \"У нас в Малом зале до сих пор проходят\"\n",
    "test(model, 300, seed, 0.1)\n",
    "test(model, 300, seed, 0.5)\n",
    "test(model, 300, seed, 0.7)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
